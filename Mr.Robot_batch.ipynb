{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574aaef-dab4-4db1-8201-ed8646fee327",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install google-cloud-aiplatform langchain pandas datasets google-api-python-client chromadb faiss-cpu faiss-cpu transformers config google-cloud-documentai google-cloud-storage pip install google.ai.generativelanguage tiktoken --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0395dd12-5c86-4834-8029-91c61e9febd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Forgive me Guido\n",
    "# \n",
    "# This notebook is a demo of a simple QnA fueled by PALM en enriched with context through embeddings. \n",
    "# \n",
    "# The embeddings are created by processing 3K+ old video game manual pdf's with DocAI and feeding them to Vertex AI embeddings.\n",
    "# This specifiek version using batch processing in DocAI and stores the pdf file on GCS \n",
    "# \n",
    "#\n",
    "# Utils\n",
    "import time\n",
    "from typing import List\n",
    "import json\n",
    "\n",
    "# Langchain\n",
    "import langchain\n",
    "from pydantic import BaseModel\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.vectorstores import Chroma\n",
    "from google.ai.generativelanguage import HarmCategory\n",
    "from google.ai.generativelanguage import SafetySetting\n",
    "\n",
    "#Document AI\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.api_core.exceptions import InternalServerError\n",
    "from google.api_core.exceptions import RetryError\n",
    "from google.cloud import storage\n",
    "\n",
    "# the normal one didn't understand sharding. I ended up not using sharing but might as well keep using this one.\n",
    "import google.cloud.documentai_v1beta3 as documentai\n",
    "\n",
    "# Variables\n",
    "#\n",
    "# DocAI\n",
    "\n",
    "project_number = 'NUMBER'\n",
    "location = 'eu' # Format is 'us' or 'eu'\n",
    "processor_id = 'fe9efa246ac573db' #  Create processor before running sample\n",
    "processor_version_id = 'pretrained-ocr-v1.0-2020-09-23' # Processor version to use\n",
    "input_mime_type = 'application/pdf' # Refer to https://cloud.google.com/document-ai/docs/file-types for supported file types\n",
    "gcs_bucket_name = 'BUCKETNAME' # Sure could construct the other variables from this one - some people prefer it like this\n",
    "gcs_input_uri = \"BUCKET_DIR\"  # Format: `gs://bucket/directory/file.pdf`` or `gs://bucket/directory/``\n",
    "field_mask = \"text\"  # Optional. The fields to return in the Document object.\n",
    "gcs_output_uri = \"gs://OUTPUT_PATH\" # this is were the processed files get stored as json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f380574-19f7-4d72-bb61-441d41d94c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stolen from: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/langchain-intro/intro_langchain_palm_api.ipynb\n",
    "#\n",
    "# Utility functions for Embeddings API with rate limiting\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    print(\"Waiting\")\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        yield\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(\".\", end=\"\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModel):\n",
    "    requests_per_minute: int\n",
    "    num_instances_per_batch: int\n",
    "\n",
    "    # Overriding embed_documents method\n",
    "    def embed_documents(self, texts: List[str]):\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        results = []\n",
    "        docs = list(texts)\n",
    "\n",
    "        while docs:\n",
    "            # Working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            head, docs = (\n",
    "                docs[: self.num_instances_per_batch],\n",
    "                docs[self.num_instances_per_batch :],\n",
    "            )\n",
    "            chunk = self.client.get_embeddings(head)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "\n",
    "        return [r.values for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1960c4-8e37-4e32-8cc0-73a91456b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stolen from: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/langchain-intro/intro_langchain_palm_api.ipynb\n",
    "# \n",
    "#  I increased the temp a little, haven't experimented with top k and p to much\n",
    "#\n",
    "# LLM model\n",
    "\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison@001\",\n",
    "    max_output_tokens=256,\n",
    "    temperature=0.5,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    "    #safety_settings = [\n",
    "    #    {\n",
    "    #        \"category\": HarmCategory.HARM_CATEGORY_VIOLENCE,\n",
    "     #       \"threshold\": SafetySetting.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "     #   }\n",
    "   # ]\n",
    ")\n",
    "# Embedding\n",
    "EMBEDDING_QPM = 60\n",
    "EMBEDDING_NUM_BATCH = 5\n",
    "embeddings = CustomVertexAIEmbeddings(\n",
    "    requests_per_minute=EMBEDDING_QPM,\n",
    "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
    "    max_output_tokens=1024 # I've changed the default to allow for more output token in the embeddings (default is like 256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf4f46-56e9-4624-bd02-48a66da5efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Straight out of our docs and some online examples. Some examples cheated by reading the results immediately and dumping\n",
    "# it into some big string in the function but I'm not doing that\n",
    "#\n",
    "#\n",
    "def batch_process_documents(\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    gcs_input_uri: str,\n",
    "    gcs_output_uri: str,\n",
    "    processor_version_id: Optional[str] = None,\n",
    "    input_mime_type: Optional[str] = None,\n",
    "    field_mask: Optional[str] = None,\n",
    "    timeout: int = 9999, # gave it a loooong timeout because we are processing a lot of files\n",
    "):\n",
    "    # You must set the api_endpoint if you use a location other than \"us\".\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "    if not gcs_input_uri.endswith(\"/\") and \".\" in gcs_input_uri:\n",
    "        # Specify specific GCS URIs to process individual documents\n",
    "        gcs_document = documentai.GcsDocument(\n",
    "            gcs_uri=gcs_input_uri, mime_type=input_mime_type\n",
    "        )\n",
    "        # Load GCS Input URI into a List of document files\n",
    "        gcs_documents = documentai.GcsDocuments(documents=[gcs_document])\n",
    "        input_config = documentai.BatchDocumentsInputConfig(gcs_documents=gcs_documents)\n",
    "    else:\n",
    "        # Specify a GCS URI Prefix to process an entire directory\n",
    "        gcs_prefix = documentai.GcsPrefix(gcs_uri_prefix=gcs_input_uri)\n",
    "        input_config = documentai.BatchDocumentsInputConfig(gcs_prefix=gcs_prefix)\n",
    "        \n",
    "    # Cloud Storage URI for the Output Directory\n",
    "    gcs_output_config = documentai.DocumentOutputConfig.GcsOutputConfig(\n",
    "        gcs_uri=gcs_output_uri, field_mask=field_mask\n",
    "    )\n",
    "    # Where to write results\n",
    "    output_config = documentai.DocumentOutputConfig(gcs_output_config=gcs_output_config)\n",
    "\n",
    "    if processor_version_id:\n",
    "        # The full resource name of the processor version, e.g.:\n",
    "        # projects/{project_number}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}\n",
    "        name = client.processor_version_path(\n",
    "            project_number, location, processor_id, processor_version_id\n",
    "        )\n",
    "    else:\n",
    "        # The full resource name of the processor, e.g.:\n",
    "        # projects/{project_number}/locations/{location}/processors/{processor_id}\n",
    "        name = client.processor_path(project_number, location, processor_id)\n",
    "\n",
    "    request = documentai.BatchProcessRequest(\n",
    "        name=name,\n",
    "        input_documents=input_config,\n",
    "        document_output_config=output_config, \n",
    "    )\n",
    "\n",
    "    # BatchProcess returns a Long Running Operation (LRO)\n",
    "    operation = client.batch_process_documents(request)\n",
    "\n",
    "    # Continually polls the operation until it is complete.\n",
    "    # This could take some time for larger files\n",
    "    # Format: projects/{project_number}/locations/{location}/operations/{operation_id}\n",
    "    try:\n",
    "        print(f\"Waiting for operation {operation.operation.name} to complete...\")\n",
    "        operation.result(timeout=timeout)\n",
    "    # Catch exception when operation doesn\"t finish before timeout\n",
    "    except (RetryError, InternalServerError) as e:\n",
    "        print(e.message)\n",
    "\n",
    "    # NOTE: Can also use callbacks for asynchronous processing\n",
    "    #\n",
    "    # def my_callback(future):\n",
    "    #   result = future.result()\n",
    "    #\n",
    "    # operation.add_done_callback(my_callback)\n",
    "\n",
    "    # Once the operation is complete,\n",
    "    # get output document information from operation metadata\n",
    "    metadata = documentai.BatchProcessMetadata(operation.metadata)\n",
    "\n",
    "    if metadata.state != documentai.BatchProcessMetadata.State.SUCCEEDED:\n",
    "        raise ValueError(f\"Batch Process Failed: {metadata.state_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e55f2-a35f-4640-b94c-cd45ed182577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function\n",
    "batch_process_documents(\n",
    "        project_number=project_number,\n",
    "        location=location,\n",
    "        processor_id=processor_id,\n",
    "        gcs_input_uri=gcs_input_uri,\n",
    "        gcs_output_uri=gcs_output_uri,\n",
    "        input_mime_type=input_mime_type,\n",
    "        field_mask=field_mask,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0d2f0f-b8f9-4b0c-bdbb-9f5a15cb6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the output files from DocAI from GCS. I'm using the document object from DocAI to easily load the JSON.\n",
    "#\n",
    "# Maybe its the old school metadata fanboy in me but I also store the all the paths as strings in the docs list\n",
    "#\n",
    "client = storage.Client()\n",
    "output_blobs = client.list_blobs(gcs_bucket_name, prefix=\"output/\")\n",
    "docs=[]\n",
    "paths=[]\n",
    "for blob in output_blobs:\n",
    "    if blob.content_type != \"application/json\":\n",
    "        print(f\"Skipping non-supported file: {blob.name} - Mimetype: {blob.content_type}\")\n",
    "        continue\n",
    "    paths.append(blob.name)\n",
    "    document=documentai.Document.from_json(blob.download_as_bytes(), ignore_unknown_fields=True)\n",
    "    docs.append(document.text)\n",
    "paths=\"\".join(paths)\n",
    "docs.append(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07a00a-ae7a-454b-a1b1-9110ed6546e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The max token size for outputs from embeddings is 1024, same as the max input token size for Palm.\n",
    "# That leaves no room for a prompt, so I'm using the recursive textsplitter to make smaller chunks. \n",
    "# Might be interesting to see the results with even smaller chunks\n",
    "#\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=800, chunk_overlap=50)\n",
    "texts = text_splitter.create_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e922c1b2-e36e-4ed0-873e-4c33fcef9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store docs in local vectorstore as index\n",
    "# it may take a while since API is rate limited\n",
    "# Also found this somewhere, added persistence for the db\n",
    "# This takes a lotta lottta lotta time\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory = \"index_batch\")\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca23854-8666-47fd-914b-32d282810de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max k as a search arguments gives us some room to experiment what works best when using embeddings. \n",
    "#\n",
    "#\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2038705a-9a02-418a-a7b2-6634f9479bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses LLM to synthesize results from the search index.\n",
    "# We use Vertex PaLM Text API for LLM\n",
    "# Create three query types so be able to test the differences\n",
    "#\n",
    "qa1 = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever\n",
    ")\n",
    "qa2 = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"map_reduce\", retriever=retriever\n",
    ")\n",
    "qa3 = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"refine\", retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "64bb0ff5-6efb-424a-a0bb-53c799b9fd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'You are Mr. Robot. Which secret agent is the best swimmer?', 'result': 'The correct answer is James Pond.\\nJames Pond is the best swimmer because he is a secret agent who is trained to swim. He is also a frogman, which means that he can breathe underwater.'}\n"
     ]
    }
   ],
   "source": [
    "# I'm sure I haven't mastered the art of prompt engineering just yet, but I like this prompt for now. I only replace the question\n",
    "# at the end and pick qa1/2/3 \n",
    "query=\"You are Mr. Robot. Which secret agent is the best swimmer?\"\n",
    "\n",
    "result = qa1({\"query\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed2391d-8904-4592-8cc6-77f6edec0f31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
