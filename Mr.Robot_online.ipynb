{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574aaef-dab4-4db1-8201-ed8646fee327",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install google-cloud-aiplatform langchain pandas datasets google-api-python-client chromadb faiss-cpu faiss-cpu transformers config google-cloud-documentai tiktoken pypdf2 --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452f4211-e55f-4791-a769-6bd618ee8b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo mkdir /share\n",
    "! sudo mount -t cifs -o username=[username],password=[password],vers=3.0 [UNC PATH TO SHARE] /share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "0395dd12-5c86-4834-8029-91c61e9febd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Forgive me Guido\n",
    "# \n",
    "# This notebook is a demo of a simple QnA fueled by PALM en enriched with context through embeddings. \n",
    "# \n",
    "# The embeddings are created by processing 3K+ old video game manual pdf's with DocAI and feeding them to Vertex AI embeddings.\n",
    "# This specifiek version add a touch of real life - the manuals are situated on a MS Windows Fileshare (on a VM in the same VPC)\n",
    "# and are real only and the files are not copied or stored anywhere else but in memory. Sad but true - \n",
    "# \n",
    "# \n",
    "#\n",
    "# Utils\n",
    "import time\n",
    "import io\n",
    "from typing import List\n",
    "from os import listdir\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "# Langchain\n",
    "import langchain\n",
    "from pydantic import BaseModel\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "#PyPDF2\n",
    "from PyPDF2 import PdfReader, PdfWriter\n",
    "\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "from langchain.chat_models import ChatVertexAI\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "\n",
    "# DocAI\n",
    "from typing import Optional\n",
    "from google.api_core.client_options import ClientOptions\n",
    "import google.cloud.documentai_v1beta3 as documentai\n",
    "\n",
    "# Variables obviously replace this will your own!\n",
    "\n",
    "# DocAI\n",
    "project_number = 'PROJECT_NUMBER' # project number, not id\n",
    "location = 'eu' # Format is 'us' or 'eu'\n",
    "processor_id = 'fe9efa246ac573db' #  Create processor before running\n",
    "processor_version_id = 'pretrained-ocr-v1.0-2020-09-23' # Processor version to use\n",
    "mime_type = 'application/pdf' # Refer to https://cloud.google.com/document-ai/docs/file-types for supported file types\n",
    "field_mask = \"text\"  # Optional. The fields to return in the Document object.\n",
    "\n",
    "# Response Processing\n",
    "max_pages = 12 # DocAI in online mode has two limits, 15 pages and 20MB, I picked 12 because that kept me consistently under both\n",
    "docs = [] # Empty list to store the return documents\n",
    "dir = 'a' # Directory that contains the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f380574-19f7-4d72-bb61-441d41d94c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stolen from: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/langchain-intro/intro_langchain_palm_api.ipynb\n",
    "#\n",
    "# Utility functions for Embeddings API with rate limiting\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    print(\"Waiting\")\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        yield\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(\".\", end=\"\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "\n",
    "class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModel):\n",
    "    requests_per_minute: int\n",
    "    num_instances_per_batch: int\n",
    "\n",
    "    # Overriding embed_documents method\n",
    "    def embed_documents(self, texts: List[str]):\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        results = []\n",
    "        docs = list(texts)\n",
    "\n",
    "        while docs:\n",
    "            # Working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            head, docs = (\n",
    "                docs[: self.num_instances_per_batch],\n",
    "                docs[self.num_instances_per_batch :],\n",
    "            )\n",
    "            chunk = self.client.get_embeddings(head)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "\n",
    "        return [r.values for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1960c4-8e37-4e32-8cc0-73a91456b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stolen from: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/langchain-intro/intro_langchain_palm_api.ipynb\n",
    "# \n",
    "#  I increased the temp a little, haven't experimented with top k and p to much\n",
    "#\n",
    "# LLM model\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison@001\",\n",
    "    max_output_tokens=256,\n",
    "    temperature=0.5,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    "    safety_setting=[\n",
    "            {\n",
    "            \"category\": glm.HarmCategory.HARM_CATEGORY_VIOLANT,\n",
    "            \"threshold\": safety_types.HarmBlockThreshold.BLOCK_ONLY_HIGH,\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Embedding\n",
    "EMBEDDING_QPM = 60\n",
    "EMBEDDING_NUM_BATCH = 5\n",
    "embeddings = CustomVertexAIEmbeddings(\n",
    "    requests_per_minute=EMBEDDING_QPM,\n",
    "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
    "    max_output_tokens=1024 # I've changed the default to allow for more output token in the embeddings (default is like 256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c2cf4f46-56e9-4624-bd02-48a66da5efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Straight out of our docs but I changed it a bit to allow for binary input (instead of a file)\n",
    "#\n",
    "# \n",
    "def process_document(\n",
    "    response_byte_stream: str,\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    mime_type: str,\n",
    "    field_mask: Optional[str] = None,\n",
    "    processor_version_id: Optional[str] = None,\n",
    ") -> None:\n",
    "    # You must set the `api_endpoint` if you use a location other than \"us\".\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "    if processor_version_id:\n",
    "        # The full resource name of the processor version, e.g.:\n",
    "        # `projects/{project_number}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}`\n",
    "        name = client.processor_version_path(\n",
    "            project_number, location, processor_id, processor_version_id\n",
    "        )\n",
    "    else:\n",
    "        # The full resource name of the processor, e.g.:\n",
    "        # `projects/{project_number}/locations/{location}/processors/{processor_id}`\n",
    "        name = client.processor_path(project_number, location, processor_id)\n",
    "    \n",
    "    # Load binary data\n",
    "    raw_document = documentai.RawDocument(content=response_byte_stream, mime_type=mime_type)\n",
    "\n",
    "    # Configure the process request\n",
    "    request = documentai.ProcessRequest(\n",
    "        name=name, raw_document=raw_document, field_mask=field_mask\n",
    "    )\n",
    "\n",
    "    result = client.process_document(request=request)\n",
    "\n",
    "    # For a full list of `Document` object attributes, reference this page:\n",
    "    # https://cloud.google.com/document-ai/docs/reference/rest/v1/Document\n",
    "    document = result.document\n",
    "    # Doing some ad hoc cleaning up of the data, anything that doesn't add to the semantic context should go really, but this is a start\n",
    "    text = document.text.replace('  ', ' ')\n",
    "    text = text.replace('  ', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "14da4724-7c6b-4c30-a32f-ff82df4feb95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# If you are asking yourself - shouldn't this be a function? It probably should! Or combined with the process_document function.\n",
    "# On the other hand, I'm not using it anywhere else and this just a demo. \n",
    "#\n",
    "# Basically what I am doing here is I'm listing the documents in a share ( which is mounted from a Windows host) and loading them as bytes.\n",
    "# I use pypdf2 to count the pages and put in some logic to create batches because we can't go over 15 pages.\n",
    "# I was to lazy to also check for filesize - if the pfd is under 12 pages it gets sent to DocAI immediately, else it will split the pdf.\n",
    "# I use a list to store all the text string returned by DocAI\n",
    "#\n",
    "# Maybe its the old school metadata fanboy in me but I also store the full_path of the file and the directory listing to the docs list\n",
    "#\n",
    "# Processing like this is not really fast. Its actually quite slow. This probably belongs in a data processing pipeline of sorts\n",
    "# or could be changed into parallel processes but I don't know how to do that yet in Python\n",
    "#\n",
    "list_dir = listdir(dir)\n",
    "docs.append(dir)\n",
    "for file_name in (list_dir):\n",
    "    #print(i)\n",
    "    full_path=dir+'/'+file_name\n",
    "    with open(full_path,\"rb\") as fh:\n",
    "        bytes_stream= io.BytesIO(fh.read())\n",
    "    reader=PdfReader(bytes_stream)\n",
    "    page_number=len(reader.pages)\n",
    "    if page_number >= max_pages:\n",
    "        page_array = np.arange(1,page_number)\n",
    "        batches= math.floor(page_number/max_pages)+1\n",
    "        batch_array = np.array_split(page_array, batches)\n",
    "        for batch in batch_array:\n",
    "            pdf_writer = PdfWriter()\n",
    "            for page_num in batch:\n",
    "                pdf_writer.add_page(reader.pages[page_num.item()])\n",
    "            response_bytes_stream = io.BytesIO()\n",
    "            pdf_writer.write(response_bytes_stream)\n",
    "            result=process_document(response_bytes_stream.getvalue(),project_number,location,processor_id,mime_type)\n",
    "            docs.append(result)\n",
    "    else:\n",
    "        result=process_document(bytes_stream.getvalue(),project_number,location,processor_id,mime_type)\n",
    "        docs.append(result)\n",
    "    docs.append(full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ff07a00a-ae7a-454b-a1b1-9110ed6546e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The max token size for outputs from embeddings is 1024, same as the max input token size for Palm.\n",
    "# That leaves no room for a prompt, so I'm using the recursive textsplitter to make smaller chunks. \n",
    "# Might be interesting to see the results with even smaller chunks\n",
    "#\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=300, chunk_overlap=50)\n",
    "documents = text_splitter.create_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e922c1b2-e36e-4ed0-873e-4c33fcef9942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting\n",
      "......................................"
     ]
    }
   ],
   "source": [
    "# Store docs in local vectorstore as index\n",
    "# it may take a while since API is rate limited\n",
    "# Also found this somewhere, added persistence for the db\n",
    "\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory = \"index\")\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "cca23854-8666-47fd-914b-32d282810de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max k as a search arguments gives us some room to experiment what works best when using embeddings. \n",
    "#\n",
    "#\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "2038705a-9a02-418a-a7b2-6634f9479bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses LLM to synthesize results from the search index.\n",
    "# We use Vertex PaLM Text API for LLM\n",
    "# Create three query types so be able to test the differences\n",
    "#\n",
    "\n",
    "qa1 = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"stuff\", retriever=retriever,\n",
    "    return_source_documents=True,\n",
    ")\n",
    "qa2 = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"map_reduce\", retriever=retriever\n",
    ")\n",
    "qa3 = RetrievalQA.from_chain_type(\n",
    "    llm=llm, chain_type=\"refine\", retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "64bb0ff5-6efb-424a-a0bb-53c799b9fd47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'You are Mr. Robot. You know everything about video games.Argos no Juujiken (Japan)?', 'result': 'Argos no Juujiken (Japan) is the Japanese name for Anticipation.', 'source_documents': [Document(page_content='EmuMovies\\nNintendo\\nNINTENDO OF AMERICA INC. PO BOX 957 REDMOND) WA 98073-0957 USA\\nPRINTED IN JAPAN\\nNES-AP USA\\nAnticipation\\n●\\nA B C D E F G H I J K L M N O P Q R S T U V W X Y Z\\nINSTRUCTION BOOKLET\\nLook for this seal on all software and accessories\\nfor your Nintendo Entertainment System. It repre-\\nsents Nintendo\\'s commitment to bringing you only\\nthe highest quality products. Items not carrying\\nthis seal have not been approved by\\nNintendo, and are not guaranteed\\nto meet our standards of\\nexcellence in workmanship,\\nreliability and most of all,\\nentertainment value.\\n20\\nTHIS SEAL IS\\nYOUR ASSURANCE THAT\\nCONFE\\nNintendo\\nHMS APPROVED AND\\nGUARANTEED THE\\nQUALITY OF THIS\\nPRODUCT.\\nThank you for selecting the Nintendo Entertainment System ANTICIPATION™* Pak.\\nOBJECT OF THE GAME/GAME DESCRIPTION\\nChallenge yourself to quickly identify video pictures drawn on the screen and\\nadvance your Game Marker around the Video Game Board. Compete against the\\ncomputer players or up to three of your friends. Don\\'t forget to keep an eye out for the\\npuzzle colors you need to advance to the next level. Anticipation has hundreds of\\npuzzles from 16 different categories designed to give you and your friends countless\\nhours of video fun.\\n1. PRECAUTIONS\\nA. This is a high precision game. It should not be stored in places that are very hot or\\ncold. Never hit it or drop it. Do not take it apart.\\nB. Avoid touching the connectors. Do not get them wet or dirty. Doing so may\\ndamage the game.\\nC. Do not clean with benzene, paint thinner, alcohol or other such solvent.\\nNOTE: In the event of product improvement, Nintendo Entertainment System specifications and design are\\nsubject to change without prior notice.\\nNintendo of America Inc. 1988 Nintendo \"Ⓒ1958 Rare, Lod. Licensed exclusively to Nintendo of America Inc.\\n2\\n2. NAME OF CONTROLLER PARTS AND OPERATING\\nINSTRUCTIONS\\n03\\n- Control Pad\\nSELECT START B\\nSELECT button\\n+Control Pad\\nSTART button\\nUsed\\nto move\\nLetter Entry Cursor left\\nUsed to answer puzzle,\\nenter letters\\nUsed to move Letter Entry\\nCursor right\\nUsed to answer puzzle\\nANTICIPATION:\\nWMLICIDYLION\\nPause:\\nL\\nBARE LTD.\\nEMILD SATAU DVITIES &\\nSELECT Button:\\nNot Used\\nSTART Button:\\nPress this button to begin.\\nIf you wish to interrupt play in the middle of a game, press the START Button. The\\npause tone will sound and the game will stop. Press the START Button again\\nwhen you wish to continue playing. The game will continue from where you left\\noff. NOTE: The Pause function will not work during a puzzle.\\n3. SETTING UP THE GAME\\nBefore beginning play, first select the number of players (you and your friends). Up\\nto four can play one game. Then select the number of computer players (note: the\\nmaximum number of players and computer players combined is four). Finally, select\\nthe skill level for your game which will optimize both challenge and fun!\\nGAME SET UP SCREEN\\n475\\nSHERIBUTION\\nMALACE KURISH OF CLANKURD\\nMUPALA UP COMPUTEA PLASIE\\nSELECT SKILL LIKVID.\\nKIN\\nWi HIDD IGH\\nPress the Control Pad right or left then press', metadata={})]}\n"
     ]
    }
   ],
   "source": [
    "# I'm sure I haven't mastered the art of prompt engineering just yet, but I like this prompt for now. I only replace the question\n",
    "# at the end and pick qa1/2/3 \n",
    "query=\"You are Mr. Robot. You know everything about video games.Argos no Juujiken (Japan)?\"\n",
    "\n",
    "result = qa1({\"query\": query})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40fbece-3719-4972-8060-32a80dbe73dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad675f1d-7ae1-406b-a851-b3b82579803e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
