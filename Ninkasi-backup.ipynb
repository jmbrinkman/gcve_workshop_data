{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574aaef-dab4-4db1-8201-ed8646fee327",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install google-cloud-aiplatform langchain pandas datasets google-api-python-client chromadb faiss-cpu faiss-cpu transformers config google-cloud-documentai google-cloud-storage tiktoken --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0395dd12-5c86-4834-8029-91c61e9febd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Forgive me Guido\n",
    "# \n",
    "# \n",
    "#\n",
    "# Utils\n",
    "import time\n",
    "from typing import List\n",
    "import json\n",
    "from os import listdir\n",
    "\n",
    "\n",
    "# Langchain\n",
    "import langchain\n",
    "from pydantic import BaseModel\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Vertex AI\n",
    "from google.cloud import aiplatform\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.schema import HumanMessage, SystemMessage\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "#Document AI\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "from google.api_core.client_options import ClientOptions\n",
    "from google.api_core.exceptions import InternalServerError\n",
    "from google.api_core.exceptions import RetryError\n",
    "from google.cloud import storage\n",
    "\n",
    "# the normal one didn't understand sharding. I ended up not using sharing but might as well keep using this one.\n",
    "import google.cloud.documentai_v1beta3 as documentai\n",
    "\n",
    "# Variables\n",
    "#\n",
    "# DocAI\n",
    "\n",
    "project_number = 'PROJECT_NUMBER'\n",
    "location = 'eu' # Format is 'us' or 'eu'\n",
    "processor_id = 'fe9efa246ac573db' #  Create processor before running sample\n",
    "processor_version_id = 'pretrained-ocr-v1.0-2020-09-23' # Processor version to use\n",
    "input_mime_type = 'application/pdf' # Refer to https://cloud.google.com/document-ai/docs/file-types for supported file types\n",
    "gcs_bucket_name = 'BUCKETNAME' # Sure could construct the other variables from this one - some people prefer it like this\n",
    "gcs_input_uri = \"BUCKET_DIR\"  # Format: `gs://bucket/directory/file.pdf`` or `gs://bucket/directory/``\n",
    "field_mask = \"text\"  # Optional. The fields to return in the Document object.\n",
    "gcs_output_uri = \"gs://OUTPUT_PATH\" # this is were the processed files get stored as json\n",
    "\n",
    "# Loading CSVs\n",
    "dir = \"gcs/csv/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2eb938-2544-4181-a93f-591663fc4535",
   "metadata": {},
   "outputs": [],
   "source": [
    "! MY_BUCKET=gcs_bucket_name\n",
    "\n",
    "! mkdir -p gcs # Create a folder that will be used as a mount point\n",
    "\n",
    "! gcsfuse $MY_BUCKET \"/home/jupyter/gcs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f380574-19f7-4d72-bb61-441d41d94c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stolen from: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/langchain-intro/intro_langchain_palm_api.ipynb\n",
    "#\n",
    "# Utility functions for Embeddings API with rate limiting\n",
    "def rate_limit(max_per_minute):\n",
    "    period = 60 / max_per_minute\n",
    "    print(\"Waiting\")\n",
    "    while True:\n",
    "        before = time.time()\n",
    "        yield\n",
    "        after = time.time()\n",
    "        elapsed = after - before\n",
    "        sleep_time = max(0, period - elapsed)\n",
    "        if sleep_time > 0:\n",
    "            print(\".\", end=\"\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "class CustomVertexAIEmbeddings(VertexAIEmbeddings, BaseModel):\n",
    "    requests_per_minute: int\n",
    "    num_instances_per_batch: int\n",
    "\n",
    "    # Overriding embed_documents method\n",
    "    def embed_documents(self, texts: List[str]):\n",
    "        limiter = rate_limit(self.requests_per_minute)\n",
    "        results = []\n",
    "        docs = list(texts)\n",
    "\n",
    "        while docs:\n",
    "            # Working in batches because the API accepts maximum 5\n",
    "            # documents per request to get embeddings\n",
    "            head, docs = (\n",
    "                docs[: self.num_instances_per_batch],\n",
    "                docs[self.num_instances_per_batch :],\n",
    "            )\n",
    "            chunk = self.client.get_embeddings(head)\n",
    "            results.extend(chunk)\n",
    "            next(limiter)\n",
    "\n",
    "        return [r.values for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1960c4-8e37-4e32-8cc0-73a91456b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stolen from: https://github.com/GoogleCloudPlatform/generative-ai/blob/main/language/examples/langchain-intro/intro_langchain_palm_api.ipynb\n",
    "# \n",
    "#  I increased the temp a little, haven't experimented with top k and p to much\n",
    "#\n",
    "# LLM model\n",
    "llm = VertexAI(\n",
    "    model_name=\"text-bison@001\",\n",
    "    max_output_tokens=256,\n",
    "    temperature=0.9,\n",
    "    top_p=0.8,\n",
    "    top_k=40,\n",
    "    verbose=True,\n",
    ")\n",
    "# Embedding\n",
    "EMBEDDING_QPM = 60\n",
    "EMBEDDING_NUM_BATCH = 5\n",
    "embeddings = CustomVertexAIEmbeddings(\n",
    "    requests_per_minute=EMBEDDING_QPM,\n",
    "    num_instances_per_batch=EMBEDDING_NUM_BATCH,\n",
    "    max_output_tokens=1024 # I've changed the default to allow for more output token in the embeddings (default is like 256)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cf4f46-56e9-4624-bd02-48a66da5efb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Straight out of our docs and some online examples. Some examples cheated by reading the results immediately and dumping\n",
    "# it into some big string in the function but I'm not doing that\n",
    "#\n",
    "#\n",
    "def batch_process_documents(\n",
    "    project_number: str,\n",
    "    location: str,\n",
    "    processor_id: str,\n",
    "    gcs_input_uri: str,\n",
    "    gcs_output_uri: str,\n",
    "    processor_version_id: Optional[str] = None,\n",
    "    input_mime_type: Optional[str] = None,\n",
    "    field_mask: Optional[str] = None,\n",
    "    timeout: int = 9999, # gave it a loooong timeout because we are processing a lot of files\n",
    "):\n",
    "    # You must set the api_endpoint if you use a location other than \"us\".\n",
    "    opts = ClientOptions(api_endpoint=f\"{location}-documentai.googleapis.com\")\n",
    "\n",
    "    client = documentai.DocumentProcessorServiceClient(client_options=opts)\n",
    "\n",
    "    if not gcs_input_uri.endswith(\"/\") and \".\" in gcs_input_uri:\n",
    "        # Specify specific GCS URIs to process individual documents\n",
    "        gcs_document = documentai.GcsDocument(\n",
    "            gcs_uri=gcs_input_uri, mime_type=input_mime_type\n",
    "        )\n",
    "        # Load GCS Input URI into a List of document files\n",
    "        gcs_documents = documentai.GcsDocuments(documents=[gcs_document])\n",
    "        input_config = documentai.BatchDocumentsInputConfig(gcs_documents=gcs_documents)\n",
    "    else:\n",
    "        # Specify a GCS URI Prefix to process an entire directory\n",
    "        gcs_prefix = documentai.GcsPrefix(gcs_uri_prefix=gcs_input_uri)\n",
    "        input_config = documentai.BatchDocumentsInputConfig(gcs_prefix=gcs_prefix)\n",
    "        \n",
    "    # Cloud Storage URI for the Output Directory\n",
    "    gcs_output_config = documentai.DocumentOutputConfig.GcsOutputConfig(\n",
    "        gcs_uri=gcs_output_uri, field_mask=field_mask\n",
    "    )\n",
    "    # Where to write results\n",
    "    output_config = documentai.DocumentOutputConfig(gcs_output_config=gcs_output_config)\n",
    "\n",
    "    if processor_version_id:\n",
    "        # The full resource name of the processor version, e.g.:\n",
    "        # projects/{project_number}/locations/{location}/processors/{processor_id}/processorVersions/{processor_version_id}\n",
    "        name = client.processor_version_path(\n",
    "            project_number, location, processor_id, processor_version_id\n",
    "        )\n",
    "    else:\n",
    "        # The full resource name of the processor, e.g.:\n",
    "        # projects/{project_number}/locations/{location}/processors/{processor_id}\n",
    "        name = client.processor_path(project_number, location, processor_id)\n",
    "\n",
    "    request = documentai.BatchProcessRequest(\n",
    "        name=name,\n",
    "        input_documents=input_config,\n",
    "        document_output_config=output_config, \n",
    "    )\n",
    "\n",
    "    # BatchProcess returns a Long Running Operation (LRO)\n",
    "    operation = client.batch_process_documents(request)\n",
    "\n",
    "    # Continually polls the operation until it is complete.\n",
    "    # This could take some time for larger files\n",
    "    # Format: projects/{project_number}/locations/{location}/operations/{operation_id}\n",
    "    try:\n",
    "        print(f\"Waiting for operation {operation.operation.name} to complete...\")\n",
    "        operation.result(timeout=timeout)\n",
    "    # Catch exception when operation doesn\"t finish before timeout\n",
    "    except (RetryError, InternalServerError) as e:\n",
    "        print(e.message)\n",
    "\n",
    "    # NOTE: Can also use callbacks for asynchronous processing\n",
    "    #\n",
    "    # def my_callback(future):\n",
    "    #   result = future.result()\n",
    "    #\n",
    "    # operation.add_done_callback(my_callback)\n",
    "\n",
    "    # Once the operation is complete,\n",
    "    # get output document information from operation metadata\n",
    "    metadata = documentai.BatchProcessMetadata(operation.metadata)\n",
    "\n",
    "    if metadata.state != documentai.BatchProcessMetadata.State.SUCCEEDED:\n",
    "        raise ValueError(f\"Batch Process Failed: {metadata.state_message}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e55f2-a35f-4640-b94c-cd45ed182577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call the function\n",
    "batch_process_documents(\n",
    "        project_number=project_number,\n",
    "        location=location,\n",
    "        processor_id=processor_id,\n",
    "        gcs_input_uri=gcs_input_uri,\n",
    "        gcs_output_uri=gcs_output_uri,\n",
    "        input_mime_type=input_mime_type,\n",
    "        field_mask=field_mask,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0d2f0f-b8f9-4b0c-bdbb-9f5a15cb6286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the output files from DocAI from GCS. I'm using the document object from DocAI to easily load the JSON.\n",
    "#\n",
    "# Maybe its the old school metadata fanboy in me but I also store the all the paths as strings in the docs list\n",
    "#\n",
    "client = storage.Client()\n",
    "output_blobs = client.list_blobs(gcs_bucket_name, prefix=\"pdf_output/\")\n",
    "docs=[]\n",
    "paths=[]\n",
    "for blob in output_blobs:\n",
    "    if blob.content_type != \"application/json\":\n",
    "        print(f\"Skipping non-supported file: {blob.name} - Mimetype: {blob.content_type}\")\n",
    "        continue\n",
    "    paths.append(blob.name)\n",
    "    document=documentai.Document.from_json(blob.download_as_bytes(), ignore_unknown_fields=True)\n",
    "    docs.append(document.text)\n",
    "paths=\"\".join(paths)\n",
    "docs.append(paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c35c8b-8183-49cf-841c-2f5b4a424899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The max token size for outputs from embeddings is 1024, same as the max input token size for Palm.\n",
    "# That leaves no room for a prompt, so I'm using the recursive textsplitter to make smaller chunks. \n",
    "# Might be interesting to see the results with even smaller chunks\n",
    "#\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=800, chunk_overlap=50)\n",
    "texts = text_splitter.create_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e922c1b2-e36e-4ed0-873e-4c33fcef9942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store docs in local vectorstore as index\n",
    "# it may take a while since API is rate limited\n",
    "# Also found this somewhere, added persistence for the db\n",
    "# This takes a lotta lottta lotta time\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory = \"index_ninkasi\")\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff07a00a-ae7a-454b-a1b1-9110ed6546e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Maybe its the old school metadata fanboy in me but I also store the all the paths as strings in the docs list\n",
    "#\n",
    "from langchain.document_loaders import CSVLoader\n",
    "\n",
    "docs=[]\n",
    "list_dir = listdir(dir)\n",
    "for file_name in (list_dir):\n",
    "    full_path=dir+'/'+file_name\n",
    "    csv = CSVLoader (full_path).load()\n",
    "    for content in csv:\n",
    "        docs.append(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca331b3-4428-4a17-aa24-ae9fe731aafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=800, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b7c5c3-ad90-4d51-b3f9-ff216c5e4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store docs in local vectorstore as index\n",
    "# it may take a while since API is rate limited\n",
    "# Also found this somewhere, added persistence for the db\n",
    "# This takes a lotta lottta lotta time\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory = \"index_ninkasi\")\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71d50ff-8df9-453f-a5dd-3632cf042700",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "dir='gcs/txt/'\n",
    "\n",
    "docs=[]\n",
    "list_dir = listdir(dir)\n",
    "for file_name in (list_dir):\n",
    "    full_path=dir+'/'+file_name\n",
    "    txt = TextLoader (full_path).load()\n",
    "    for content in txt:\n",
    "        docs.append(content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcf1268-f983-4644-bf77-aaed679b3b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=800, chunk_overlap=50)\n",
    "texts = text_splitter.split_documents(docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7019d0b-aceb-4024-9b5b-4d7365b6d599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store docs in local vectorstore as index\n",
    "# it may take a while since API is rate limited\n",
    "# Also found this somewhere, added persistence for the db\n",
    "# This takes a lotta lottta lotta time\n",
    "db = Chroma.from_documents(texts, embeddings, persist_directory = \"index_ninkasi\")\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca23854-8666-47fd-914b-32d282810de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max k as a search arguments gives us some room to experiment what works best when using embeddings. \n",
    "#\n",
    "#\n",
    "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89dceff-f916-4c39-b381-4bd1201e364b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "prompt_template = \"\"\"Use the context below create a recipe of max 1000 words with special ingredients for a beer with type below:\n",
    "    Context: {context}\n",
    "    Type: {type}\n",
    "    recipe:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"type\"])\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453506b9-c438-46e9-a4fe-0b1a3c989b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recipe(type):\n",
    "    docs = db.similarity_search(type, k=1)\n",
    "    inputs = [{\"context\": doc.page_content, \"type\": type} for doc in docs]\n",
    "    print(chain.apply(inputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3679bb-c0a8-4ae6-8401-ae081f0dff14",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_recipe('Meh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6fd651-0b18-4752-8954-a450255ddcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "collection = chroma_client.create_collection(name=\"reeb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb537a7-5285-4fbc-b9a9-92a445818519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "\n",
    "chroma_client.list_collections()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2c6aeae4-66ce-4512-9ac3-7a7e5e58313a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Name: Dampf Loc\\nkey: 1157\\nStyle: California Common / Steam Beer\\nStyle Key: 29\\nBrewery: Local Option\\nDescription: Notes:Dampf Loc is an all-barley warm fermented ale brewed by the folks from Chicago’s Local Option; inspired by “Dampfbier” (literally “steam beer”) originally crafted by medieval peasant inhabitants of southeast Bavaria. During the fermentation process generous amounts of foam and surface bubbles burst in the tank, giving the illusion that the ferment is boiling or “steaming.” Tony Russomanno and Noah Hopkins took it upon themselves to brew Dampf Loc as a modern interpretation of its Teutonic predecessor, creating a “Hefe Gerste” ale that is true to its roots; unmistakable in character yet accessible to beer drinkers of all palates.Dampf Loc has a mild hop profile, medium body with pleasant effervescence and a clear, rich, orange hue. Dampf Loc is ideal for warm months, yet enjoyable year round.\\nABV: 5.3\\nAve Rating: 3.68\\nMin IBU: 35\\nMax IBU: 45\\nAstringency: 30\\nBody: 56\\nAlcohol: 28\\nBitter: 44\\nSweet: 77\\nSour: 50\\nSalty: 6\\nFruits: 46\\nHoppy: 73\\nSpices: 16\\nMalty: 148', metadata={'source': 'gcs/csv//beer_data_set.csv', 'row': 906})]\n"
     ]
    }
   ],
   "source": [
    "print(db.similarity_search(\"dank\", k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d50ed0d-5f5d-4252-97d3-560d2e07f6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4540d6f6-3f22-45c9-b5ee-ec9be4e07cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import requests\n",
    "\n",
    "base_url = 'https://api.brewersfriend.com/v1/recipes/'\n",
    "headers = {'Accept': 'application/json','X-API-KEY':'API_KEY'}\n",
    "\n",
    "\n",
    "df = pd.read_csv('gcs/recipedata.csv')\n",
    "\n",
    "df = df.assign(URL = lambda x: x['URL'].str.extract('(\\d+)'))\n",
    "\n",
    "recipes=[]\n",
    "\n",
    "for recipe in df.URL:\n",
    "    url=base_url+recipe+\".xml\"\n",
    "    req = requests.get(url, headers=headers, auth=auth)\n",
    "    file = \"xml/\"+recipe+\".xml\"\n",
    "    f = open(file, \"w\")\n",
    "    f.write(req.text)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c385b8-2331-4532-9bdc-a0fbb179b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baee951a-2cf6-47c6-98b3-d03943c41f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0735b49e-e8ef-4c28-b20a-f9f4859974ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(req.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de792a56-b8cf-4b1a-b103-fc9ce03af948",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
